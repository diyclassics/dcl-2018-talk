{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NN'), ('World', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Default tagger\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('NN')\n",
    "\n",
    "print(tagger.tag('Hello World'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3914 tagged sentences in treebank.\n",
      "[[('Neither', 'DT'), ('Lorillard', 'NNP'), ('nor', 'CC'), ('the', 'DT'), ('researchers', 'NNS'), ('who', 'WP'), ('*T*-3', '-NONE-'), ('studied', 'VBD'), ('the', 'DT'), ('workers', 'NNS'), ('were', 'VBD'), ('aware', 'JJ'), ('of', 'IN'), ('any', 'DT'), ('research', 'NN'), ('on', 'IN'), ('smokers', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Kent', 'NNP'), ('cigarettes', 'NNS'), ('.', '.')], [('``', '``'), ('We', 'PRP'), ('have', 'VBP'), ('no', 'DT'), ('useful', 'JJ'), ('information', 'NN'), ('on', 'IN'), ('whether', 'IN'), ('users', 'NNS'), ('are', 'VBP'), ('at', 'IN'), ('risk', 'NN'), (',', ','), (\"''\", \"''\"), ('said', 'VBD'), ('*T*-1', '-NONE-'), ('James', 'NNP'), ('A.', 'NNP'), ('Talcott', 'NNP'), ('of', 'IN'), ('Boston', 'NNP'), (\"'s\", 'POS'), ('Dana-Farber', 'NNP'), ('Cancer', 'NNP'), ('Institute', 'NNP'), ('.', '.')]]\n",
      "\n",
      "\n",
      "Sentence 1:\n",
      "Neither Lorillard nor the researchers who *T*-3 studied the workers were aware of any research on smokers of the Kent cigarettes .\n",
      "\n",
      "Sentence 2:\n",
      "`` We have no useful information on whether users are at risk , '' said *T*-1 James A. Talcott of Boston 's Dana-Farber Cancer Institute .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up train/test sents\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import untag\n",
    "\n",
    "\n",
    "print(f'There are {len(treebank.tagged_sents())} tagged sentences in treebank.')\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "example_sent = treebank.sents()[3000]\n",
    "test_sents = treebank.tagged_sents()[3001:]\n",
    "\n",
    "sent_slice = slice(10,12)\n",
    "print(train_sents[sent_slice])\n",
    "print('\\n')\n",
    "\n",
    "for i, sent in enumerate(train_sents[sent_slice]):\n",
    "    print(f'Sentence {i+1}:')\n",
    "#     print(f'{\" \".join([word for word, _ in sent])}\\n')\n",
    "    print(f'{\" \".join(untag(sent))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', None), ('World', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Unigram tagger w. training data\n",
    "# Note that are also bigram, trigram, etc. taggers, but they will not prove to be useful for lemmatization\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "tagger = UnigramTagger(train_sents)\n",
    "\n",
    "print(tagger.tag('Hello World'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('Tokyo', 'NNP'), (',', ','), ('the', 'DT'), ('Nikkei', None), ('index', 'NN'), ('of', 'IN'), ('225', 'CD'), ('selected', None), ('issues', 'NNS'), (',', ','), ('which', 'WDT'), ('*T*-1', '-NONE-'), ('gained', 'VBD'), ('132', None), ('points', 'NNS'), ('Tuesday', 'NNP'), (',', ','), ('added', 'VBD'), ('14.99', None), ('points', 'NNS'), ('to', 'TO'), ('35564.43', None), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag(example_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagger accuracy: 85.72%\n"
     ]
    }
   ],
   "source": [
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', None), ('Tokyo', None), (',', None), ('the', None), ('Nikkei', 'NNP'), ('index', None), ('of', None), ('225', None), ('selected', 'VBN'), ('issues', None), (',', None), ('which', None), ('*T*-1', None), ('gained', None), ('132', None), ('points', None), ('Tuesday', None), (',', None), ('added', None), ('14.99', None), ('points', None), ('to', None), ('35564.43', None), ('.', None)]\n",
      "\n",
      "\n",
      "Tagger accuracy: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# Unigram Tagger w. dictionary\n",
    "\n",
    "tagger = UnigramTagger(model={'Nikkei': 'NNP', 'selected': 'VBN'})\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('Tokyo', 'NNP'), (',', ','), ('the', 'DT'), ('Nikkei', 'NN'), ('index', 'NN'), ('of', 'IN'), ('225', 'CD'), ('selected', 'NN'), ('issues', 'NNS'), (',', ','), ('which', 'WDT'), ('*T*-1', '-NONE-'), ('gained', 'VBD'), ('132', 'NN'), ('points', 'NNS'), ('Tuesday', 'NNP'), (',', ','), ('added', 'VBD'), ('14.99', 'NN'), ('points', 'NNS'), ('to', 'TO'), ('35564.43', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "Tagger accuracy: 87.42%\n"
     ]
    }
   ],
   "source": [
    "# Backoff tagging\n",
    "\n",
    "backoff_tagger = DefaultTagger('NN')\n",
    "tagger = UnigramTagger(train_sents, backoff=backoff_tagger)\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', None), ('Tokyo', None), (',', None), ('the', None), ('Nikkei', None), ('index', None), ('of', None), ('225', 'CD'), ('selected', 'VBD'), ('issues', None), (',', None), ('which', None), ('*T*-1', None), ('gained', 'VBD'), ('132', 'CD'), ('points', None), ('Tuesday', None), (',', None), ('added', 'VBD'), ('14.99', 'CD'), ('points', None), ('to', None), ('35564.43', 'CD'), ('.', None)]\n",
      "\n",
      "\n",
      "Tagger accuracy: 5.36%\n"
     ]
    }
   ],
   "source": [
    "# Regex tagging\n",
    "\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "patterns = [\n",
    "    (r'\\b\\d+\\b', 'CD'),\n",
    "    (r'\\b.+ed\\b', 'VBD')\n",
    "]\n",
    "\n",
    "tagger = RegexpTagger(patterns)\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'),\n",
      " ('Tokyo', 'NNP'),\n",
      " (',', ','),\n",
      " ('the', 'DT'),\n",
      " ('Nikkei', 'NNP'),\n",
      " ('index', 'NN'),\n",
      " ('of', 'IN'),\n",
      " ('225', 'CD'),\n",
      " ('selected', 'VBN'),\n",
      " ('issues', 'NNS'),\n",
      " (',', ','),\n",
      " ('which', 'WDT'),\n",
      " ('*T*-1', '-NONE-'),\n",
      " ('gained', 'VBD'),\n",
      " ('132', None),\n",
      " ('points', 'NNS'),\n",
      " ('Tuesday', 'NNP'),\n",
      " (',', ','),\n",
      " ('added', 'VBD'),\n",
      " ('14.99', None),\n",
      " ('points', 'NNS'),\n",
      " ('to', 'TO'),\n",
      " ('35564.43', None),\n",
      " ('.', '.')]\n",
      "\n",
      "\n",
      "Tagger accuracy: 85.73%\n"
     ]
    }
   ],
   "source": [
    "# Another backoff chain\n",
    "\n",
    "# default_tagger = DefaultTagger('NN')\n",
    "default_tagger = None\n",
    "train_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "dict_tagger = UnigramTagger(model={'Nikkei': 'NNP', 'selected': 'VBN'}, backoff=train_tagger)\n",
    "tagger = dict_tagger\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization as a backoff task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"περὶ πολλοῦ ἂν ποιησαίμην ὦ ἄνδρες τὸ τοιούτους ὑμᾶς ἐμοὶ δικαστὰς περὶ τούτου τοῦ πράγματος γενέσθαι οἷοίπερ ἂν ὑμῖν αὐτοῖς εἴητε τοιαῦτα πεπονθότες\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('περὶ', 'Unk'), ('πολλοῦ', 'Unk'), ('ἂν', 'Unk'), ('ποιησαίμην', 'Unk'), ('ὦ', 'Unk'), ('ἄνδρες', 'Unk'), ('τὸ', 'Unk'), ('τοιούτους', 'Unk'), ('ὑμᾶς', 'Unk'), ('ἐμοὶ', 'Unk'), ('δικαστὰς', 'Unk'), ('περὶ', 'Unk'), ('τούτου', 'Unk'), ('τοῦ', 'Unk'), ('πράγματος', 'Unk'), ('γενέσθαι', 'Unk'), ('οἷοίπερ', 'Unk'), ('ἂν', 'Unk'), ('ὑμῖν', 'Unk'), ('αὐτοῖς', 'Unk'), ('εἴητε', 'Unk'), ('τοιαῦτα', 'Unk'), ('πεπονθότες', 'Unk')]\n"
     ]
    }
   ],
   "source": [
    "# Default lemmatizer\n",
    "\n",
    "from cltk.lemmatize.greek.backoff import DefaultLemmatizer\n",
    "lemmatizer = DefaultLemmatizer('Unk')\n",
    "\n",
    "print(lemmatizer.lemmatize(test.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33555 tagged sentences in treebank.\n",
      "[[('ἔνθα', 'ἔνθα'), ('δὲ', 'δέ'), ('πολλαὶ', 'πολύς'), ('ψυχαὶ', 'ψυχή'), ('ἐλεύσονται', 'ἔρχομαι'), ('νεκύων', 'νέκυς'), ('κατατεθνηώτων', 'καταθνήσκω'), ('.', '.')], [('Ἀριστογείτων', 'Ἀριστογείτων'), ('δὲ', 'δέ'), ('ἐν', 'ἐν'), ('τῷ', 'ὁ'), ('κατὰ', 'κατά'), ('Φρύνης', 'φρύνη'), ('τὸ', 'ὁ'), ('κύριόν', 'κύριος'), ('φησιν', 'φημί'), ('αὐτῆς', 'αὐτός'), ('εἶναι', 'εἰμί'), ('ὄνομα', 'ὄνομα'), ('Μνησαρέτην', 'Μνησαρέτην'), ('.', '.')]]\n",
      "\n",
      "\n",
      "Sentence 1:\n",
      "ἔνθα δὲ πολλαὶ ψυχαὶ ἐλεύσονται νεκύων κατατεθνηώτων .\n",
      "\n",
      "Sentence 2:\n",
      "Ἀριστογείτων δὲ ἐν τῷ κατὰ Φρύνης τὸ κύριόν φησιν αὐτῆς εἶναι ὄνομα Μνησαρέτην .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up train/test sents\n",
    "\n",
    "import pickle\n",
    "tagged_sents = pickle.load(open(\"../data/tagged_sents.p\", \"rb\" ))\n",
    "\n",
    "print(f'There are {len(tagged_sents)} tagged sentences in treebank.')\n",
    "\n",
    "from random import Random\n",
    "Random(4).shuffle(tagged_sents)\n",
    "\n",
    "train_sents = tagged_sents[1:30000]\n",
    "example_sent = untag(tagged_sents[0])\n",
    "test_sents = tagged_sents[30000:]\n",
    "\n",
    "sent_slice = slice(10,12)\n",
    "print(train_sents[sent_slice])\n",
    "print('\\n')\n",
    "\n",
    "for i, sent in enumerate(train_sents[sent_slice]):\n",
    "    print(f'Sentence {i+1}:')\n",
    "#     print(f'{\" \".join([word for word, _ in sent])}\\n')\n",
    "    print(f'{\" \".join(untag(sent))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὸ', 'ὁ'), ('μὲν', 'μέν'), ('αὖτις', 'αὖθις'), ('ἐὺς', 'ἐύς'), ('πάις', 'παῖς'), ('Ἰαπετοῖο', 'Ἰαπέτος'), ('ἔκλεψ̓', None), ('ἀνθρώποισι', 'ἄνθρωπος'), ('Διὸς', 'Ζεύς'), ('πάρα', 'παρά'), ('μητιόεντος', None), ('ἐν', 'ἐν'), ('κοῒλῳ', None), ('νάρθηκι', None), ('λαθὼν', 'λανθάνω'), ('Δία', 'Ζεύς'), ('τερπικέραυνον', 'τερπικέραυνος'), ('.', '.')]\n",
      "\n",
      "\n",
      "Lemmatizer accuracy: 88.20%\n"
     ]
    }
   ],
   "source": [
    "# Unigram lemmatizer w. training data\n",
    "# Note that are also bigram, trigram, etc. taggers, but they will not prove to be useful for lemmatization\n",
    "\n",
    "from cltk.lemmatize.greek.backoff import UnigramLemmatizer\n",
    "lemmatizer = UnigramLemmatizer(train_sents)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὸ', None), ('μὲν', None), ('αὖτις', None), ('ἐὺς', None), ('πάις', None), ('Ἰαπετοῖο', None), ('ἔκλεψ̓', None), ('ἀνθρώποισι', None), ('Διὸς', None), ('πάρα', None), ('μητιόεντος', 'μητιόεις'), ('ἐν', None), ('κοῒλῳ', None), ('νάρθηκι', 'νάρθηξ'), ('λαθὼν', None), ('Δία', None), ('τερπικέραυνον', None), ('.', None)]\n",
      "\n",
      "\n",
      "Lemmatizer accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Unigram lemmatizer w. dictionary\n",
    "\n",
    "lemmatizer = UnigramLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'})\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὸ', None), ('μὲν', None), ('αὖτις', None), ('ἐὺς', None), ('πάις', None), ('Ἰαπετοῖο', None), ('ἔκλεψ̓', None), ('ἀνθρώποισι', None), ('Διὸς', None), ('πάρα', None), ('μητιόεντος', 'μητιόεις'), ('ἐν', None), ('κοῒλῳ', None), ('νάρθηκι', None), ('λαθὼν', None), ('Δία', None), ('τερπικέραυνον', None), ('.', None)]\n",
      "\n",
      "\n",
      "Lemmatizer accuracy: 0.06%\n"
     ]
    }
   ],
   "source": [
    "# Regex lemmatizing\n",
    "\n",
    "from cltk.lemmatize.greek.backoff import RegexpLemmatizer\n",
    "\n",
    "greek_sub_patterns = [\n",
    "('(ό)(εις|εντος|εντι|εντα)$', r'\\1εις'),\n",
    "]\n",
    "    \n",
    "lemmatizer = RegexpLemmatizer(greek_sub_patterns)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὸ', 'ὁ'), ('μὲν', 'μέν'), ('αὖτις', 'αὖθις'), ('ἐὺς', 'ἐύς'), ('πάις', 'παῖς'), ('Ἰαπετοῖο', 'Ἰαπέτος'), ('ἔκλεψ̓', 'Unknown'), ('ἀνθρώποισι', 'ἄνθρωπος'), ('Διὸς', 'Ζεύς'), ('πάρα', 'παρά'), ('μητιόεντος', 'μητιόεις'), ('ἐν', 'ἐν'), ('κοῒλῳ', 'Unknown'), ('νάρθηκι', 'νάρθηξ'), ('λαθὼν', 'λανθάνω'), ('Δία', 'Ζεύς'), ('τερπικέραυνον', 'τερπικέραυνος'), ('.', 'punc')]\n",
      "\n",
      "\n",
      "Lemmatizer accuracy: 76.92%\n"
     ]
    }
   ],
   "source": [
    "# Backoff lemmatizing\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "lemmatizer_5 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = UnigramLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = UnigramLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = UnigramLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in GREEK_MODEL.items():\n",
    "    if value=='punc':\n",
    "        GREEK_MODEL[key] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὸ', 'ὁ'), ('μὲν', 'μέν'), ('αὖτις', 'αὖθις'), ('ἐὺς', 'ἐύς'), ('πάις', 'παῖς'), ('Ἰαπετοῖο', 'Ἰαπέτος'), ('ἔκλεψ̓', 'Unknown'), ('ἀνθρώποισι', 'ἄνθρωπος'), ('Διὸς', 'Ζεύς'), ('πάρα', 'παρά'), ('μητιόεντος', 'μητιόεις'), ('ἐν', 'ἐν'), ('κοῒλῳ', 'Unknown'), ('νάρθηκι', 'νάρθηξ'), ('λαθὼν', 'λανθάνω'), ('Δία', 'Ζεύς'), ('τερπικέραυνον', 'τερπικέραυνος'), ('.', '.')]\n",
      "\n",
      "\n",
      "Lemmatizer accuracy: 87.71%\n"
     ]
    }
   ],
   "source": [
    "# Backoff lemmatizing; importance of model definition\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "for key, value in GREEK_MODEL.items():\n",
    "    if value=='punc':\n",
    "        GREEK_MODEL[key] = key\n",
    "\n",
    "lemmatizer_5 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = UnigramLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = UnigramLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = UnigramLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Wrappers to Backoff Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MorpheusWebserviceLemmatizer as subclass of NLTK's Sequential Backoff Tagger\n",
    "\n",
    "from lxml import etree as ET\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import string\n",
    "\n",
    "import betacode.conv\n",
    "\n",
    "from nltk.tag.sequential import SequentialBackoffTagger, UnigramTagger\n",
    "# from cltk.lemmatize.backoff import UnigramLemmatizer, RegexpLemmatizer\n",
    "from cltk.lemmatize.backoff import RegexpLemmatizer\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "class MorpheusWebserviceLemmatizer(SequentialBackoffTagger):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, backoff=None):\n",
    "        \"\"\"Setup for MorpheusWebserviceLemmatizer\"\"\"\n",
    "        SequentialBackoffTagger.__init__(self, backoff)       \n",
    "        \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        \"\"\"Returns a lemma for the token at a given index\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :param index: Int with current token\n",
    "        :param history: List with tokens that have already been lemmatized\n",
    "        :return: String, spec. the lemma found at the current index.\n",
    "        \"\"\"  \n",
    "        token = self._prep_token(tokens[index])\n",
    "        return self._lemmatize(token)\n",
    "    \n",
    "    def _lemmatize(self, token):\n",
    "        with urlopen(f'http://www.perseus.tufts.edu/hopper/xmlmorph?lang=greek&lookup={token}') as f:\n",
    "            tree = ET.parse(f)\n",
    "            root = tree.getroot()\n",
    "        lemmas = root.findall('.//lemma')\n",
    "        if lemmas:\n",
    "            return [lemma.text for lemma in lemmas][0]\n",
    "    \n",
    "    def _prep_token(self, token):\n",
    "        punctuation = string.punctuation + '\t̓”“‘᾽（）'\n",
    "        numbers = '0123456789'\n",
    "        table = str.maketrans({key: None for key in punctuation+numbers})\n",
    "        return betacode.conv.uni_to_beta(token).translate(table)\n",
    "    \n",
    "    def lemmatize(self, tokens, return_all=True):\n",
    "        return self.tag(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split UnigramLemmatizer into two more clearly named taggers\n",
    "\n",
    "class DictionaryLemmatizer(UnigramTagger):\n",
    "    \"\"\"Setup for UnigramLemmatizer()\"\"\"\n",
    "    def __init__(self, train=None, model=None, backoff=None, cutoff=0):\n",
    "        \"\"\"\"\"\"\n",
    "        UnigramTagger.__init__(self, train=None, model=model, backoff=backoff, cutoff=cutoff)\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        return self.tag(tokens)\n",
    "    \n",
    "    \n",
    "class TrainLemmatizer(UnigramTagger):\n",
    "    \"\"\"Setup for UnigramLemmatizer()\"\"\"\n",
    "    def __init__(self, train=None, model=None, backoff=None, cutoff=0):\n",
    "        \"\"\"\"\"\"\n",
    "        UnigramTagger.__init__(self, train=train, model=None, backoff=backoff, cutoff=cutoff)\n",
    "    \n",
    "\n",
    "    def _train(self, tagged_corpus, cutoff=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize this ContextTagger's ``_context_to_tag`` table\n",
    "        based on the given training data.  In particular, for each\n",
    "        context ``c`` in the training data, set\n",
    "        ``_context_to_tag[c]`` to the most frequent tag for that\n",
    "        context.  However, exclude any contexts that are already\n",
    "        tagged perfectly by the backoff tagger(s).\n",
    "\n",
    "        The old value of ``self._context_to_tag`` (if any) is discarded.\n",
    "\n",
    "        :param tagged_corpus: A tagged corpus.  Each item should be\n",
    "            a list of (word, tag tuples.\n",
    "        :param cutoff: If the most likely tag for a context occurs\n",
    "            fewer than cutoff times, then exclude it from the\n",
    "            context-to-tag table for the new tagger.\n",
    "        \"\"\"\n",
    "        token_count = hit_count = 0\n",
    "\n",
    "        # A context is considered 'useful' if it's not already tagged\n",
    "        # perfectly by the backoff tagger.\n",
    "        useful_contexts = set()\n",
    "        # Count how many times each tag occurs in each context.\n",
    "        fd = ConditionalFreqDist()\n",
    "        for sentence in tagged_corpus:\n",
    "            tokens_, tags = zip(*sentence)\n",
    "            for index, (token, tag) in enumerate(sentence):\n",
    "                # Record the event.\n",
    "                token_count += 1\n",
    "                context = self.context(tokens_, index, tags[:index])\n",
    "                if context is None:\n",
    "                    continue\n",
    "                fd[context][tag] += 1\n",
    "                \n",
    "                # THE IF STATEMENT HERE HAD TO BE REMOVED—OVERLOADING TOKENS VARIABLE???!!!\n",
    "                # STILL NOT EXACTLY SURE WHY???\n",
    "                useful_contexts.add(context)\n",
    "\n",
    "        # Build the context_to_tag table -- for each context, figure\n",
    "        # out what the most likely tag is.  Only include contexts that\n",
    "        # we've seen at least `cutoff` times.\n",
    "        for context in useful_contexts:\n",
    "            best_tag = fd[context].max()\n",
    "\n",
    "            hits = fd[context][best_tag]\n",
    "            if hits > cutoff:\n",
    "                self._context_to_tag[context] = best_tag\n",
    "                hit_count += hits\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        return self.tag(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὸ', 'ὁ'), ('μὲν', 'μέν'), ('αὖτις', 'αὖθις'), ('ἐὺς', 'ἐύς'), ('πάις', 'παῖς'), ('Ἰαπετοῖο', 'Ἰαπέτος'), ('ἔκλεψ̓', 'ἐκλέπω'), ('ἀνθρώποισι', 'ἄνθρωπος'), ('Διὸς', 'Ζεύς'), ('πάρα', 'παρά'), ('μητιόεντος', 'μητιόεις'), ('ἐν', 'ἐν'), ('κοῒλῳ', 'κοῖλος'), ('νάρθηκι', 'νάρθηξ'), ('λαθὼν', 'λανθάνω'), ('Δία', 'Ζεύς'), ('τερπικέραυνον', 'τερπικέραυνος'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Backoff lemmatizing; importance of model definition\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "# print('\\n')\n",
    "# print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents[:5]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('τὸ', 'ὁ'), ('μὲν', 'μέν'), ('αὖτις', 'αὖθις'), ('ἐὺς', 'ἐύς'), ('πάις', 'παῖς'), ('Ἰαπετοῖο', 'Ἰαπέτος'), ('ἔκλεψ̓', 'ἐκλέπω'), ('ἀνθρώποισι', 'ἄνθρωπος'), ('Διὸς', 'Ζεύς'), ('πάρα', 'παρά'), ('μητιόεντος', 'μητιόεις'), ('ἐν', 'ἐν'), ('κοῒλῳ', 'κοῖλος'), ('νάρθηκι', 'νάρθηξ'), ('λαθὼν', 'λανθάνω'), ('Δία', 'Ζεύς'), ('τερπικέραυνον', 'τερπικέραυνος'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Backoff lemmatizing; importance of model definition\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "# print(example_sent)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "# print('\\n')\n",
    "# print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents[:5]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "iliad24 = \"\"\"λῦτο δ᾽ ἀγών λαοὶ δὲ θοὰς ἐπὶ νῆας ἕκαστοι\n",
    "ἐσκίδναντ᾽ ἰέναι\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('λῦτο', 'λύω'),\n",
      " ('δ᾽', 'δέ'),\n",
      " ('ἀγών', 'ἀγών'),\n",
      " ('λαοὶ', 'λαός'),\n",
      " ('δὲ', 'δέ'),\n",
      " ('θοὰς', 'θοός'),\n",
      " ('ἐπὶ', 'ἐπί'),\n",
      " ('νῆας', 'ναῦς'),\n",
      " ('ἕκαστοι', 'ἕκαστος'),\n",
      " ('ἐσκίδναντ᾽', 'σκίδνημι'),\n",
      " ('ἰέναι', 'εἶμι')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lemmatizer.lemmatize(iliad24.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "republic1 = \"\"\"κατέβην χθὲς εἰς Πειραιᾶ μετὰ Γλαύκωνος τοῦ Ἀρίστωνος προσευξόμενός τε τῇ θεῷ καὶ ἅμα τὴν ἑορτὴν βουλόμενος θεάσασθαι τίνα τρόπον ποιήσουσιν ἅτε νῦν πρῶτον ἄγοντες\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('κατέβην', 'καταβαίνω'),\n",
      " ('χθὲς', 'χθές'),\n",
      " ('εἰς', 'εἰμί'),\n",
      " ('Πειραιᾶ', 'Πειραιεύς'),\n",
      " ('μετὰ', 'μετά'),\n",
      " ('Γλαύκωνος', 'Unknown'),\n",
      " ('τοῦ', 'ὁ'),\n",
      " ('Ἀρίστωνος', 'Ἀρίστων'),\n",
      " ('προσευξόμενός', 'προσεύχομαι'),\n",
      " ('τε', 'τε'),\n",
      " ('τῇ', 'ὁ'),\n",
      " ('θεῷ', 'θεός'),\n",
      " ('καὶ', 'καί'),\n",
      " ('ἅμα', 'ἅμα'),\n",
      " ('τὴν', 'ὁ'),\n",
      " ('ἑορτὴν', 'ἑορτή'),\n",
      " ('βουλόμενος', 'βούλομαι'),\n",
      " ('θεάσασθαι', 'θεάομαι'),\n",
      " ('τίνα', 'τις'),\n",
      " ('τρόπον', 'τρόπος'),\n",
      " ('ποιήσουσιν', 'ποιέω'),\n",
      " ('ἅτε', 'ἅτε'),\n",
      " ('νῦν', 'νῦν'),\n",
      " ('πρῶτον', 'πρῶτος'),\n",
      " ('ἄγοντες', 'ἄγω')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lemmatizer.lemmatize(republic1.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_1_14 = \"\"\"Καὶ μετὰ τὸ παραδοθῆναι τὸν Ἰωάνην ἦλθεν ὁ Ἰησοῦς εἰς τὴν Γαλιλαίαν κηρύσσων τὸ εὐαγγέλιον τοῦ θεοῦ καὶ λέγων ὅτι Πεπλήρωται ὁ καιρὸς καὶ ἤγγικεν ἡ βασιλεία τοῦ θεοῦ μετανοεῖτε καὶ πιστεύετε ἐν τῷ εὐαγγελίῳ\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Καὶ', 'καί'),\n",
      " ('μετὰ', 'μετά'),\n",
      " ('τὸ', 'ὁ'),\n",
      " ('παραδοθῆναι', 'παραδίδωμι'),\n",
      " ('τὸν', 'ὁ'),\n",
      " ('Ἰωάνην', 'Unknown'),\n",
      " ('ἦλθεν', 'ἔρχομαι'),\n",
      " ('ὁ', 'ὁ'),\n",
      " ('Ἰησοῦς', 'Ἰησοῦς'),\n",
      " ('εἰς', 'εἰμί'),\n",
      " ('τὴν', 'ὁ'),\n",
      " ('Γαλιλαίαν', 'Unknown'),\n",
      " ('κηρύσσων', 'κηρύσσω'),\n",
      " ('τὸ', 'ὁ'),\n",
      " ('εὐαγγέλιον', 'εὐαγγέλιον'),\n",
      " ('τοῦ', 'ὁ'),\n",
      " ('θεοῦ', 'θεός'),\n",
      " ('καὶ', 'καί'),\n",
      " ('λέγων', 'λέγω'),\n",
      " ('ὅτι', 'ὅτι'),\n",
      " ('Πεπλήρωται', 'πληρόω'),\n",
      " ('ὁ', 'ὁ'),\n",
      " ('καιρὸς', 'καιρός'),\n",
      " ('καὶ', 'καί'),\n",
      " ('ἤγγικεν', 'Unknown'),\n",
      " ('ἡ', 'ὁ'),\n",
      " ('βασιλεία', 'βασιλεία'),\n",
      " ('τοῦ', 'ὁ'),\n",
      " ('θεοῦ', 'θεός'),\n",
      " ('μετανοεῖτε', 'μετανοέω'),\n",
      " ('καὶ', 'καί'),\n",
      " ('πιστεύετε', 'πιστεύω'),\n",
      " ('ἐν', 'ἐν'),\n",
      " ('τῷ', 'ὁ'),\n",
      " ('εὐαγγελίῳ', 'εὐαγγέλιον')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lemmatizer.lemmatize(mark_1_14.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing; customize chain for Plato\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "custom_dict = {'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}\n",
    "\n",
    "greek_sub_patterns.append(('(ων)(ος|ι|α)$', r'\\1'))\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model=custom_dict, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('κατέβην', 'καταβαίνω'),\n",
      " ('χθὲς', 'χθές'),\n",
      " ('εἰς', 'εἰμί'),\n",
      " ('Πειραιᾶ', 'Πειραιεύς'),\n",
      " ('μετὰ', 'μετά'),\n",
      " ('Γλαύκωνος', 'Γλαύκων'),\n",
      " ('τοῦ', 'ὁ'),\n",
      " ('Ἀρίστωνος', 'Ἀρίστων'),\n",
      " ('προσευξόμενός', 'προσεύχομαι'),\n",
      " ('τε', 'τε'),\n",
      " ('τῇ', 'ὁ'),\n",
      " ('θεῷ', 'θεός'),\n",
      " ('καὶ', 'καί'),\n",
      " ('ἅμα', 'ἅμα'),\n",
      " ('τὴν', 'ὁ'),\n",
      " ('ἑορτὴν', 'ἑορτή'),\n",
      " ('βουλόμενος', 'βούλομαι'),\n",
      " ('θεάσασθαι', 'θεάομαι'),\n",
      " ('τίνα', 'τις'),\n",
      " ('τρόπον', 'τρόπος'),\n",
      " ('ποιήσουσιν', 'ποιέω'),\n",
      " ('ἅτε', 'ἅτε'),\n",
      " ('νῦν', 'νῦν'),\n",
      " ('πρῶτον', 'πρῶτος'),\n",
      " ('ἄγοντες', 'ἄγω')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lemmatizer.lemmatize(republic1.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing; customize chain for Plato\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "custom_dict = {'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}\n",
    "custom_dict.update({'ἤγγικεν': 'ἐγγιζω'})\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model=custom_dict, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Καὶ', 'καί'),\n",
      " ('μετὰ', 'μετά'),\n",
      " ('τὸ', 'ὁ'),\n",
      " ('παραδοθῆναι', 'παραδίδωμι'),\n",
      " ('τὸν', 'ὁ'),\n",
      " ('Ἰωάνην', 'Unknown'),\n",
      " ('ἦλθεν', 'ἔρχομαι'),\n",
      " ('ὁ', 'ὁ'),\n",
      " ('Ἰησοῦς', 'Ἰησοῦς'),\n",
      " ('εἰς', 'εἰμί'),\n",
      " ('τὴν', 'ὁ'),\n",
      " ('Γαλιλαίαν', 'Unknown'),\n",
      " ('κηρύσσων', 'κηρύσσω'),\n",
      " ('τὸ', 'ὁ'),\n",
      " ('εὐαγγέλιον', 'εὐαγγέλιον'),\n",
      " ('τοῦ', 'ὁ'),\n",
      " ('θεοῦ', 'θεός'),\n",
      " ('καὶ', 'καί'),\n",
      " ('λέγων', 'λέγω'),\n",
      " ('ὅτι', 'ὅτι'),\n",
      " ('Πεπλήρωται', 'πληρόω'),\n",
      " ('ὁ', 'ὁ'),\n",
      " ('καιρὸς', 'καιρός'),\n",
      " ('καὶ', 'καί'),\n",
      " ('ἤγγικεν', 'ἐγγιζω'),\n",
      " ('ἡ', 'ὁ'),\n",
      " ('βασιλεία', 'βασιλεία'),\n",
      " ('τοῦ', 'ὁ'),\n",
      " ('θεοῦ', 'θεός'),\n",
      " ('μετανοεῖτε', 'μετανοέω'),\n",
      " ('καὶ', 'καί'),\n",
      " ('πιστεύετε', 'πιστεύω'),\n",
      " ('ἐν', 'ἐν'),\n",
      " ('τῷ', 'ὁ'),\n",
      " ('εὐαγγελίῳ', 'εὐαγγέλιον')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lemmatizer.lemmatize(mark_1_14.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer accuracy: 94.80%\n"
     ]
    }
   ],
   "source": [
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents[:250]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
