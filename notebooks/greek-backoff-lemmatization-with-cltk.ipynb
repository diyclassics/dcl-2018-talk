{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backoff Lemmatization for Ancient Greek with the Classical Language Toolkit\n",
    "### Patrick J. Burns, Institute for the Study of the Ancient World\n",
    "\n",
    "[diyclassics.github.io](diyclassics.github.io) | [@diyclassics](twitter.com/diyclassics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An iPython notebook written to introduce seminar participants to the code behind backoff tagging, backoff lemmatization, and related matters.\n",
    "\n",
    "Presented at the Institute for Classical Studies as part of the Digital Classics London Summer Seminar on 7.27.18.\n",
    "\n",
    "Main repo at: [https://github.com/diyclassics/dcl-2018-talk](https://github.com/diyclassics/dcl-2018-talk)\n",
    "\n",
    "Last updated 7.27.18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary Python packages (not necessary for working in Binder):\n",
    "\n",
    "- pip install jupyter\n",
    "- pip install -e git+https://github.com/diyclassics/cltk.git@lemma-update#egg=cltk (see https://pip.readthedocs.io/en/1.1/usage.html#version-control-systems on install packages from forks/branches)\n",
    "\n",
    "Necessary NLTK corpora\n",
    "- tagged_sents (installed below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default tagger\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('NN')\n",
    "\n",
    "print(tagger.tag('Hello World'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up train/test sents\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import untag\n",
    "\n",
    "\n",
    "print(f'There are {len(treebank.tagged_sents())} tagged sentences in treebank.')\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "example_sent = treebank.sents()[3000]\n",
    "test_sents = treebank.tagged_sents()[3001:]\n",
    "\n",
    "sent_slice = slice(10,12)\n",
    "print(train_sents[sent_slice])\n",
    "print('\\n')\n",
    "\n",
    "for i, sent in enumerate(train_sents[sent_slice]):\n",
    "    print(f'Sentence {i+1}:')\n",
    "#     print(f'{\" \".join([word for word, _ in sent])}\\n')\n",
    "    print(f'{\" \".join(untag(sent))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram tagger w. training data\n",
    "# Note that are also bigram, trigram, etc. taggers, but they will not prove to be useful for lemmatization\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "tagger = UnigramTagger(train_sents)\n",
    "\n",
    "print(tagger.tag('Hello World'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tagger.tag(example_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram Tagger w. dictionary\n",
    "\n",
    "tagger = UnigramTagger(model={'Nikkei': 'NNP', 'selected': 'VBN'})\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff tagging\n",
    "\n",
    "backoff_tagger = DefaultTagger('NN')\n",
    "tagger = UnigramTagger(train_sents, backoff=backoff_tagger)\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex tagging\n",
    "\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "patterns = [\n",
    "    (r'\\b\\d+\\b', 'CD'),\n",
    "    (r'\\b.+ed\\b', 'VBD')\n",
    "]\n",
    "\n",
    "tagger = RegexpTagger(patterns)\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another backoff chain\n",
    "\n",
    "# default_tagger = DefaultTagger('NN')\n",
    "default_tagger = None\n",
    "train_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "dict_tagger = UnigramTagger(model={'Nikkei': 'NNP', 'selected': 'VBN'}, backoff=train_tagger)\n",
    "tagger = dict_tagger\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(f'Tagger accuracy: {tagger.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization as a backoff task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test selection; Lysias 1.1\n",
    "\n",
    "test = \"\"\"περὶ πολλοῦ ἂν ποιησαίμην ὦ ἄνδρες τὸ τοιούτους ὑμᾶς ἐμοὶ δικαστὰς περὶ τούτου τοῦ πράγματος γενέσθαι οἷοίπερ ἂν ὑμῖν αὐτοῖς εἴητε τοιαῦτα πεπονθότες\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default lemmatizer\n",
    "\n",
    "from cltk.lemmatize.greek.backoff import DefaultLemmatizer\n",
    "lemmatizer = DefaultLemmatizer('Unk')\n",
    "\n",
    "print(lemmatizer.lemmatize(test.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up train/test sents\n",
    "\n",
    "import pickle\n",
    "tagged_sents = pickle.load(open(\"../data/tagged_sents.p\", \"rb\" ))\n",
    "\n",
    "print(f'There are {len(tagged_sents)} tagged sentences in treebank.')\n",
    "\n",
    "from random import Random\n",
    "Random(4).shuffle(tagged_sents)\n",
    "\n",
    "train_sents = tagged_sents[1:30000]\n",
    "example_sent = untag(tagged_sents[0])\n",
    "test_sents = tagged_sents[30000:]\n",
    "\n",
    "sent_slice = slice(10,12)\n",
    "print(train_sents[sent_slice])\n",
    "print('\\n')\n",
    "\n",
    "for i, sent in enumerate(train_sents[sent_slice]):\n",
    "    print(f'Sentence {i+1}:')\n",
    "#     print(f'{\" \".join([word for word, _ in sent])}\\n')\n",
    "    print(f'{\" \".join(untag(sent))}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram lemmatizer w. training data\n",
    "# Note that are also bigram, trigram, etc. taggers, but they will not prove to be useful for lemmatization\n",
    "\n",
    "from cltk.lemmatize.greek.backoff import UnigramLemmatizer\n",
    "lemmatizer = UnigramLemmatizer(train_sents)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram lemmatizer w. dictionary\n",
    "\n",
    "lemmatizer = UnigramLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'})\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex lemmatizing\n",
    "\n",
    "from cltk.lemmatize.greek.backoff import RegexpLemmatizer\n",
    "\n",
    "greek_sub_patterns = [\n",
    "    ('(ό)(εις|εντος|εντι|εντα)$', r'\\1εις'),\n",
    "]\n",
    "    \n",
    "lemmatizer = RegexpLemmatizer(greek_sub_patterns)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "lemmatizer_5 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = UnigramLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = UnigramLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer   = UnigramLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize punctuation\n",
    "\n",
    "for key, value in GREEK_MODEL.items():\n",
    "    if value=='punc':\n",
    "        GREEK_MODEL[key] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing; importance of model definition\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "for key, value in GREEK_MODEL.items():\n",
    "    if value=='punc':\n",
    "        GREEK_MODEL[key] = key\n",
    "\n",
    "lemmatizer_5 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = UnigramLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = UnigramLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = UnigramLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "print('\\n')\n",
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Wrappers to Backoff Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MorpheusWebserviceLemmatizer as subclass of NLTK's Sequential Backoff Tagger\n",
    "\n",
    "from lxml import etree as ET\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import string\n",
    "\n",
    "import betacode.conv\n",
    "\n",
    "from nltk.tag.sequential import SequentialBackoffTagger, UnigramTagger\n",
    "# from cltk.lemmatize.backoff import UnigramLemmatizer, RegexpLemmatizer\n",
    "from cltk.lemmatize.backoff import RegexpLemmatizer\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "class MorpheusWebserviceLemmatizer(SequentialBackoffTagger):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, backoff=None):\n",
    "        \"\"\"Setup for MorpheusWebserviceLemmatizer\"\"\"\n",
    "        SequentialBackoffTagger.__init__(self, backoff)       \n",
    "        \n",
    "    def choose_tag(self, tokens, index, history):\n",
    "        \"\"\"Returns a lemma for the token at a given index\n",
    "        :param tokens: List of tokens to be lemmatized\n",
    "        :param index: Int with current token\n",
    "        :param history: List with tokens that have already been lemmatized\n",
    "        :return: String, spec. the lemma found at the current index.\n",
    "        \"\"\"  \n",
    "        token = self._prep_token(tokens[index])\n",
    "        return self._lemmatize(token)\n",
    "    \n",
    "    def _lemmatize(self, token):\n",
    "        with urlopen(f'http://www.perseus.tufts.edu/hopper/xmlmorph?lang=greek&lookup={token}') as f:\n",
    "            tree = ET.parse(f)\n",
    "            root = tree.getroot()\n",
    "        lemmas = root.findall('.//lemma')\n",
    "        if lemmas:\n",
    "            return [lemma.text for lemma in lemmas][0]\n",
    "    \n",
    "    def _prep_token(self, token):\n",
    "        punctuation = string.punctuation + '\t̓”“‘᾽（）'\n",
    "        numbers = '0123456789'\n",
    "        table = str.maketrans({key: None for key in punctuation+numbers})\n",
    "        return betacode.conv.uni_to_beta(token).translate(table)\n",
    "    \n",
    "    def lemmatize(self, tokens, return_all=True):\n",
    "        return self.tag(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split UnigramLemmatizer into two more clearly named taggers\n",
    "\n",
    "class DictionaryLemmatizer(UnigramTagger):\n",
    "    \"\"\"Setup for UnigramLemmatizer()\"\"\"\n",
    "    def __init__(self, train=None, model=None, backoff=None, cutoff=0):\n",
    "        \"\"\"\"\"\"\n",
    "        UnigramTagger.__init__(self, train=None, model=model, backoff=backoff, cutoff=cutoff)\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        return self.tag(tokens)\n",
    "    \n",
    "    \n",
    "class TrainLemmatizer(UnigramTagger):\n",
    "    \"\"\"Setup for UnigramLemmatizer()\"\"\"\n",
    "    def __init__(self, train=None, model=None, backoff=None, cutoff=0):\n",
    "        \"\"\"\"\"\"\n",
    "        UnigramTagger.__init__(self, train=train, model=None, backoff=backoff, cutoff=cutoff)\n",
    "    \n",
    "\n",
    "    def _train(self, tagged_corpus, cutoff=0, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize this ContextTagger's ``_context_to_tag`` table\n",
    "        based on the given training data.  In particular, for each\n",
    "        context ``c`` in the training data, set\n",
    "        ``_context_to_tag[c]`` to the most frequent tag for that\n",
    "        context.  However, exclude any contexts that are already\n",
    "        tagged perfectly by the backoff tagger(s).\n",
    "\n",
    "        The old value of ``self._context_to_tag`` (if any) is discarded.\n",
    "\n",
    "        :param tagged_corpus: A tagged corpus.  Each item should be\n",
    "            a list of (word, tag tuples.\n",
    "        :param cutoff: If the most likely tag for a context occurs\n",
    "            fewer than cutoff times, then exclude it from the\n",
    "            context-to-tag table for the new tagger.\n",
    "        \"\"\"\n",
    "        token_count = hit_count = 0\n",
    "\n",
    "        # A context is considered 'useful' if it's not already tagged\n",
    "        # perfectly by the backoff tagger.\n",
    "        useful_contexts = set()\n",
    "        # Count how many times each tag occurs in each context.\n",
    "        fd = ConditionalFreqDist()\n",
    "        for sentence in tagged_corpus:\n",
    "            tokens_, tags = zip(*sentence)\n",
    "            for index, (token, tag) in enumerate(sentence):\n",
    "                # Record the event.\n",
    "                token_count += 1\n",
    "                context = self.context(tokens_, index, tags[:index])\n",
    "                if context is None:\n",
    "                    continue\n",
    "                fd[context][tag] += 1\n",
    "                \n",
    "                # THE IF STATEMENT HERE HAD TO BE REMOVED—OVERLOADING TOKENS VARIABLE???!!!\n",
    "                # STILL NOT EXACTLY SURE WHY???\n",
    "                useful_contexts.add(context)\n",
    "\n",
    "        # Build the context_to_tag table -- for each context, figure\n",
    "        # out what the most likely tag is.  Only include contexts that\n",
    "        # we've seen at least `cutoff` times.\n",
    "        for context in useful_contexts:\n",
    "            best_tag = fd[context].max()\n",
    "\n",
    "            hits = fd[context][best_tag]\n",
    "            if hits > cutoff:\n",
    "                self._context_to_tag[context] = best_tag\n",
    "                hit_count += hits\n",
    "    \n",
    "    def lemmatize(self, tokens):\n",
    "        return self.tag(tokens)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing; importance of model definition\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer   = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "# print('\\n')\n",
    "# print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents[:5]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing; importance of model definition\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model={'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer   = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)\n",
    "\n",
    "# print(example_sent)\n",
    "\n",
    "print(lemmatizer.lemmatize(example_sent))\n",
    "# print('\\n')\n",
    "# print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents[:5]):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iliad24 = \"\"\"λῦτο δ᾽ ἀγών λαοὶ δὲ θοὰς ἐπὶ νῆας ἕκαστοι\n",
    "ἐσκίδναντ᾽ ἰέναι\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lemmatizer.lemmatize(iliad24.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "republic1 = \"\"\"κατέβην χθὲς εἰς Πειραιᾶ μετὰ Γλαύκωνος τοῦ Ἀρίστωνος προσευξόμενός τε τῇ θεῷ καὶ ἅμα τὴν ἑορτὴν βουλόμενος θεάσασθαι τίνα τρόπον ποιήσουσιν ἅτε νῦν πρῶτον ἄγοντες\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lemmatizer.lemmatize(republic1.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_1_14 = \"\"\"Καὶ μετὰ τὸ παραδοθῆναι τὸν Ἰωάνην ἦλθεν ὁ Ἰησοῦς εἰς τὴν Γαλιλαίαν κηρύσσων τὸ εὐαγγέλιον τοῦ θεοῦ καὶ λέγων ὅτι Πεπλήρωται ὁ καιρὸς καὶ ἤγγικεν ἡ βασιλεία τοῦ θεοῦ μετανοεῖτε καὶ πιστεύετε ἐν τῷ εὐαγγελίῳ\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lemmatizer.lemmatize(mark_1_14.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing; customize chain for Plato\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "custom_dict = {'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}\n",
    "\n",
    "greek_sub_patterns.append(('(ων)(ος|ι|α)$', r'\\1'))\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model=custom_dict, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lemmatizer.lemmatize(republic1.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff lemmatizing; customize chain for Plato\n",
    "\n",
    "from cltk.lemmatize.greek.greek_model import GREEK_MODEL\n",
    "custom_dict = {'μητιόεντος': 'μητιόεις', 'νάρθηκι': 'νάρθηξ'}\n",
    "custom_dict.update({'ἤγγικεν': 'ἐγγιζω'})\n",
    "\n",
    "lemmatizer_6 = DefaultLemmatizer('Unknown')\n",
    "lemmatizer_5 = MorpheusWebserviceLemmatizer(backoff=lemmatizer_6)\n",
    "lemmatizer_4 = RegexpLemmatizer(greek_sub_patterns, backoff=lemmatizer_5)\n",
    "lemmatizer_3 = DictionaryLemmatizer(model=custom_dict, backoff=lemmatizer_4)\n",
    "lemmatizer_2 = TrainLemmatizer(train_sents, backoff=lemmatizer_3)\n",
    "lemmatizer = DictionaryLemmatizer(model=GREEK_MODEL, backoff=lemmatizer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lemmatizer.lemmatize(mark_1_14.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lemmatizer accuracy: {lemmatizer.evaluate(test_sents[:250]):.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
