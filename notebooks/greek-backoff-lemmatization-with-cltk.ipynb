{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NN'), ('World', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Default tagger\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('NN')\n",
    "\n",
    "print(tagger.tag('Hello World'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3914 tagged sentences in treebank.\n",
      "[[('Neither', 'DT'), ('Lorillard', 'NNP'), ('nor', 'CC'), ('the', 'DT'), ('researchers', 'NNS'), ('who', 'WP'), ('*T*-3', '-NONE-'), ('studied', 'VBD'), ('the', 'DT'), ('workers', 'NNS'), ('were', 'VBD'), ('aware', 'JJ'), ('of', 'IN'), ('any', 'DT'), ('research', 'NN'), ('on', 'IN'), ('smokers', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Kent', 'NNP'), ('cigarettes', 'NNS'), ('.', '.')], [('``', '``'), ('We', 'PRP'), ('have', 'VBP'), ('no', 'DT'), ('useful', 'JJ'), ('information', 'NN'), ('on', 'IN'), ('whether', 'IN'), ('users', 'NNS'), ('are', 'VBP'), ('at', 'IN'), ('risk', 'NN'), (',', ','), (\"''\", \"''\"), ('said', 'VBD'), ('*T*-1', '-NONE-'), ('James', 'NNP'), ('A.', 'NNP'), ('Talcott', 'NNP'), ('of', 'IN'), ('Boston', 'NNP'), (\"'s\", 'POS'), ('Dana-Farber', 'NNP'), ('Cancer', 'NNP'), ('Institute', 'NNP'), ('.', '.')]]\n",
      "\n",
      "\n",
      "Sentence 1:\n",
      "Neither Lorillard nor the researchers who *T*-3 studied the workers were aware of any research on smokers of the Kent cigarettes .\n",
      "\n",
      "Sentence 2:\n",
      "`` We have no useful information on whether users are at risk , '' said *T*-1 James A. Talcott of Boston 's Dana-Farber Cancer Institute .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up train/test sents\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "from nltk.tag import untag\n",
    "\n",
    "\n",
    "print(f'There are {len(treebank.tagged_sents())} tagged sentences in treebank.')\n",
    "\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "example_sent = treebank.sents()[3000]\n",
    "test_sents = treebank.tagged_sents()[3001:]\n",
    "\n",
    "sent_slice = slice(10,12)\n",
    "print(train_sents[sent_slice])\n",
    "print('\\n')\n",
    "\n",
    "for i, sent in enumerate(train_sents[sent_slice]):\n",
    "    print(f'Sentence {i+1}:')\n",
    "#     print(f'{\" \".join([word for word, _ in sent])}\\n')\n",
    "    print(f'{\" \".join(untag(sent))}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', None), ('World', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Unigram tagger w. training data\n",
    "# Note that are also bigram, trigram, etc. taggers, but they will not prove to be useful for lemmatization\n",
    "\n",
    "from nltk.tag import UnigramTagger\n",
    "tagger = UnigramTagger(train_sents)\n",
    "\n",
    "print(tagger.tag('Hello World'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('Tokyo', 'NNP'), (',', ','), ('the', 'DT'), ('Nikkei', None), ('index', 'NN'), ('of', 'IN'), ('225', 'CD'), ('selected', None), ('issues', 'NNS'), (',', ','), ('which', 'WDT'), ('*T*-1', '-NONE-'), ('gained', 'VBD'), ('132', None), ('points', 'NNS'), ('Tuesday', 'NNP'), (',', ','), ('added', 'VBD'), ('14.99', None), ('points', 'NNS'), ('to', 'TO'), ('35564.43', None), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagger.tag(example_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8572231104965213\n"
     ]
    }
   ],
   "source": [
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', None), ('Tokyo', None), (',', None), ('the', None), ('Nikkei', 'NNP'), ('index', None), ('of', None), ('225', None), ('selected', 'VBN'), ('issues', None), (',', None), ('which', None), ('*T*-1', None), ('gained', None), ('132', None), ('points', None), ('Tuesday', None), (',', None), ('added', None), ('14.99', None), ('points', None), ('to', None), ('35564.43', None), ('.', None)]\n",
      "\n",
      "\n",
      "8.642668856142777e-05\n"
     ]
    }
   ],
   "source": [
    "# Unigram Tagger w. dictionary\n",
    "\n",
    "tagger = UnigramTagger(model={'Nikkei': 'NNP', 'selected': 'VBN'})\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'), ('Tokyo', 'NNP'), (',', ','), ('the', 'DT'), ('Nikkei', 'NN'), ('index', 'NN'), ('of', 'IN'), ('225', 'CD'), ('selected', 'NN'), ('issues', 'NNS'), (',', ','), ('which', 'WDT'), ('*T*-1', '-NONE-'), ('gained', 'VBD'), ('132', 'NN'), ('points', 'NNS'), ('Tuesday', 'NNP'), (',', ','), ('added', 'VBD'), ('14.99', 'NN'), ('points', 'NNS'), ('to', 'TO'), ('35564.43', 'NN'), ('.', '.')]\n",
      "\n",
      "\n",
      "0.8742059547988419\n"
     ]
    }
   ],
   "source": [
    "# Backoff tagging\n",
    "\n",
    "backoff_tagger = DefaultTagger('NN')\n",
    "tagger = UnigramTagger(train_sents, backoff=backoff_tagger)\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', None), ('Tokyo', None), (',', None), ('the', None), ('Nikkei', None), ('index', None), ('of', None), ('225', 'CD'), ('selected', 'VBD'), ('issues', None), (',', None), ('which', None), ('*T*-1', None), ('gained', 'VBD'), ('132', 'CD'), ('points', None), ('Tuesday', None), (',', None), ('added', 'VBD'), ('14.99', 'CD'), ('points', None), ('to', None), ('35564.43', 'CD'), ('.', None)]\n",
      "\n",
      "\n",
      "0.053584546908085215\n"
     ]
    }
   ],
   "source": [
    "# Regex tagging\n",
    "\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "patterns = [\n",
    "    (r'\\b\\d+\\b', 'CD'),\n",
    "    (r'\\b.+ed\\b', 'VBD')\n",
    "]\n",
    "\n",
    "tagger = RegexpTagger(patterns)\n",
    "\n",
    "print(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('At', 'IN'),\n",
      " ('Tokyo', 'NNP'),\n",
      " (',', ','),\n",
      " ('the', 'DT'),\n",
      " ('Nikkei', 'NNP'),\n",
      " ('index', 'NN'),\n",
      " ('of', 'IN'),\n",
      " ('225', 'CD'),\n",
      " ('selected', 'VBN'),\n",
      " ('issues', 'NNS'),\n",
      " (',', ','),\n",
      " ('which', 'WDT'),\n",
      " ('*T*-1', '-NONE-'),\n",
      " ('gained', 'VBD'),\n",
      " ('132', None),\n",
      " ('points', 'NNS'),\n",
      " ('Tuesday', 'NNP'),\n",
      " (',', ','),\n",
      " ('added', 'VBD'),\n",
      " ('14.99', None),\n",
      " ('points', 'NNS'),\n",
      " ('to', 'TO'),\n",
      " ('35564.43', None),\n",
      " ('.', '.')]\n",
      "\n",
      "\n",
      "0.8573095371850827\n"
     ]
    }
   ],
   "source": [
    "# Another backoff chain\n",
    "\n",
    "# default_tagger = DefaultTagger('NN')\n",
    "default_tagger = None\n",
    "train_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "dict_tagger = UnigramTagger(model={'Nikkei': 'NNP', 'selected': 'VBN'}, backoff=train_tagger)\n",
    "tagger = dict_tagger\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(tagger.tag(example_sent))\n",
    "print('\\n')\n",
    "print(tagger.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization as a backoff task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
